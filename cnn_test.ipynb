{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfc65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63019472",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87104577",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d15dd",
   "metadata": {},
   "source": [
    "### preprocessing tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947e01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfrom = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835dc02",
   "metadata": {},
   "source": [
    "### loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016a9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['plane', 'car','bird','cat','deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144782a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./dataset',train=True,download=True,transform=transfrom)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./dataset',train=False,download=True,transform=transfrom)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batchsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batchsize, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6dfa5",
   "metadata": {},
   "source": [
    "# Design CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb64fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_size = (32,32)\n",
    "        self.w = 32\n",
    "        self.h = 32\n",
    "        self.input_channels = 3\n",
    "        self.kernal_size = 3\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv2d(self.input_channels, 32 , self.kernal_size); self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv_layer2 = nn.Conv2d(32, 64 , self.kernal_size)\n",
    "        self.conv_layer3 = nn.Conv2d(64, 64 , self.kernal_size) \n",
    "\n",
    "        self.fc1 = nn.Linear(64*4*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv_layer1(x)))\n",
    "        x = self.pool(F.relu(self.conv_layer2(x)))\n",
    "        x = (F.relu(self.conv_layer3(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb600691",
   "metadata": {},
   "source": [
    "### create the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b98452a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2303b5",
   "metadata": {},
   "source": [
    "#### define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e0cab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963de2aa",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e90141c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: f0.445\n",
      "[2] loss: f0.423\n",
      "[3] loss: f0.403\n",
      "[4] loss: f0.380\n",
      "[5] loss: f0.370\n",
      "[6] loss: f0.351\n",
      "[7] loss: f0.339\n",
      "[8] loss: f0.319\n",
      "[9] loss: f0.309\n",
      "[10] loss: f0.296\n",
      "Finished Training !\n"
     ]
    }
   ],
   "source": [
    "n_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Forward prop\n",
    "        outputs = cnn_model(imgs)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_func(outputs,labels)\n",
    "\n",
    "        # Backward prop\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"[{epoch+1}] loss: f{running_loss/n_steps:.3f}\")\n",
    "\n",
    "print(\"Finished Training !\")\n",
    "\n",
    "path = './latest_cnn.pth'\n",
    "\n",
    "torch.save(cnn_model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8def52a",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788edc7",
   "metadata": {},
   "source": [
    "### Load the saved model paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "680a3efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_layer2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_layer3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_cnn = CNN()\n",
    "loaded_cnn.load_state_dict(torch.load(path))\n",
    "loaded_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d376a",
   "metadata": {},
   "source": [
    "### Calculate accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6400d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 71.54 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = len(test_loader.dataset)\n",
    "\n",
    "    for imgs, labels in test_loader:\n",
    "\n",
    "        outputs = loaded_cnn(imgs)\n",
    "\n",
    "        _, preds = torch.max(outputs,1)\n",
    "        \n",
    "        n_correct += (preds == labels).sum().item()\n",
    "\n",
    "    \n",
    "    acc =(n_correct / n_samples) * 100.0\n",
    "    print(f\"Model Accuracy = {acc} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55479c",
   "metadata": {},
   "source": [
    "# Test on examples out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb975020",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cdb6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = new_transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6558e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: plane\n",
      "Prediction: dog\n",
      "Prediction: frog\n"
     ]
    }
   ],
   "source": [
    "images_paths  = [\"airplane_ex1.jpeg\",\"dog_ex2.jpeg\",\"frog_ex3.jpeg\"]\n",
    "images = [load_img(img) for img in images_paths]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img in images:\n",
    "        outputs = loaded_cnn(img)\n",
    "        _, pred = torch.max(outputs,1)\n",
    "        print(f\"Prediction: {class_names[pred.item()]}\")        \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
