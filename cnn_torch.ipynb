{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfc65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52d6ac",
   "metadata": {},
   "source": [
    "# Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418a222",
   "metadata": {},
   "source": [
    "\n",
    "Here, we define some **hyperparameters** that control training:\n",
    "- `batchsize = 64` → number of samples processed before updating model weights.\n",
    "- `learning_rate = 0.001` → step size for gradient descent.\n",
    "- `epochs = 20` → number of complete passes through the entire training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63019472",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87104577",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d15dd",
   "metadata": {},
   "source": [
    "### preprocessing tranforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa10da",
   "metadata": {},
   "source": [
    "\n",
    "We define an image transformation pipeline using `torchvision.transforms.Compose`.  \n",
    "- `ToTensor()` converts PIL images to PyTorch tensors with values in `[0,1]`.\n",
    "- `Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))` shifts and scales pixel values to `[-1,1]` for each RGB channel, helping the network train faster and more stably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947e01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfrom = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835dc02",
   "metadata": {},
   "source": [
    "### loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9025d",
   "metadata": {},
   "source": [
    "\n",
    "Here we load the **CIFAR-10 dataset**, a standard dataset of 60,000 32×32 color images in 10 classes (like airplanes, cars, cats, etc.).\n",
    "\n",
    "- `train=True` loads the training split (50,000 images).  \n",
    "- `train=False` loads the test split (10,000 images).  \n",
    "- `download=True` ensures it’s downloaded if not already available.  \n",
    "- The `transform` applies our preprocessing pipeline to each image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c18ee3",
   "metadata": {},
   "source": [
    "\n",
    "We wrap datasets in **DataLoaders**:\n",
    "- `batch_size=batchsize` determines how many samples are fed per batch.\n",
    "- `shuffle=True` randomizes training samples to improve generalization.\n",
    "- `shuffle=False` in the test set keeps order consistent for evaluation.\n",
    "\n",
    "These loaders efficiently feed data to the model during training/testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016a9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['plane', 'car','bird','cat','deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144782a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:32<00:00, 1.84MB/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./dataset',train=True,download=True,transform=transfrom)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./dataset',train=False,download=True,transform=transfrom)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batchsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batchsize, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6dfa5",
   "metadata": {},
   "source": [
    "# Design CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fc0a3",
   "metadata": {},
   "source": [
    "\n",
    "This defines the **CNN architecture** by subclassing `nn.Module`.\n",
    "\n",
    "- Input images: 32×32 with 3 channels (RGB).\n",
    "- **Conv Layer 1:** 3→32 filters with 3×3 kernels.  \n",
    "- **Pooling:** `MaxPool2d(2,2)` halves the spatial dimensions.  \n",
    "- **Conv Layer 2:** 32→64 filters, same kernel size.  \n",
    "- **Conv Layer 3:** Another 64→64 layer for deeper feature extraction.  \n",
    "- After convolutions, we flatten features into a vector of size `64×4×4` before feeding into:\n",
    "  - **fc1:** Fully connected layer with 64 neurons.\n",
    "  - **fc2:** Output layer with 10 neurons (CIFAR-10 classes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4a0d4",
   "metadata": {},
   "source": [
    "\n",
    "Defines the **forward pass** — how data flows through the network:\n",
    "\n",
    "1. Apply **Conv1 → ReLU → Pooling**  \n",
    "2. Apply **Conv2 → ReLU → Pooling**\n",
    "3. Apply **Conv3 → ReLU** (no pooling, preserving more detail)\n",
    "4. **Flatten** the 3D feature maps into a 1D vector per image.\n",
    "5. Pass through **fc1 → ReLU**.\n",
    "6. Output **fc2**, giving raw class scores (logits) for 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb64fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_size = (32,32)\n",
    "        self.w = 32\n",
    "        self.h = 32\n",
    "        self.input_channels = 3\n",
    "        self.kernal_size = 3\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv2d(self.input_channels, 32 , self.kernal_size); self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv_layer2 = nn.Conv2d(32, 64 , self.kernal_size)\n",
    "        self.conv_layer3 = nn.Conv2d(64, 64 , self.kernal_size) \n",
    "\n",
    "        self.fc1 = nn.Linear(64*4*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv_layer1(x)))\n",
    "        x = self.pool(F.relu(self.conv_layer2(x)))\n",
    "        x = (F.relu(self.conv_layer3(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb600691",
   "metadata": {},
   "source": [
    "## Create the model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffff1c",
   "metadata": {},
   "source": [
    "\n",
    "We create an instance of our CNN model.  \n",
    "At this point, it has random weights and is ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98452a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2303b5",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b905d6",
   "metadata": {},
   "source": [
    "\n",
    "We set up:\n",
    "- **Loss Function:** `CrossEntropyLoss` — combines softmax + negative log-likelihood, suitable for multi-class classification.\n",
    "- **Optimizer:** `Adam` — an adaptive optimizer that updates model parameters using gradients and the defined learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0cab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963de2aa",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903080f7",
   "metadata": {},
   "source": [
    "This is the **training loop**:\n",
    "\n",
    "- For each **epoch** (full pass through dataset):\n",
    "  - Iterate through `train_loader` batches.\n",
    "  - **Forward pass:** feed images → get predictions.\n",
    "  - **Loss calculation:** compare predictions vs. true labels.\n",
    "  - **Backward pass:** \n",
    "    - `zero_grad()` clears old gradients.\n",
    "    - `backward()` computes new gradients.\n",
    "    - `step()` updates weights.\n",
    "  - Track running loss for monitoring.\n",
    "- After all epochs → print average loss → training complete!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befad813",
   "metadata": {},
   "source": [
    "\n",
    "Saves the **trained model parameters** (weights only) to a `.pth` file for later use.  \n",
    "This allows us to reload the trained model without retraining from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90141c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: f1.517\n",
      "[2] loss: f1.172\n",
      "[3] loss: f0.999\n",
      "[4] loss: f0.895\n",
      "[5] loss: f0.810\n",
      "[6] loss: f0.751\n",
      "[7] loss: f0.701\n",
      "[8] loss: f0.652\n",
      "[9] loss: f0.609\n",
      "[10] loss: f0.582\n",
      "[11] loss: f0.543\n",
      "[12] loss: f0.507\n",
      "[13] loss: f0.484\n",
      "[14] loss: f0.454\n",
      "[15] loss: f0.428\n",
      "[16] loss: f0.408\n",
      "[17] loss: f0.376\n",
      "[18] loss: f0.358\n",
      "[19] loss: f0.331\n",
      "[20] loss: f0.312\n",
      "Finished Training !\n"
     ]
    }
   ],
   "source": [
    "n_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Forward prop\n",
    "        outputs = cnn_model(imgs)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_func(outputs,labels)\n",
    "\n",
    "        # Backward prop\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"[{epoch+1}] loss: {running_loss/n_steps:.3f}\")\n",
    "\n",
    "print(\"Finished Training !\")\n",
    "\n",
    "path = './models/latest_cnn.pth'\n",
    "\n",
    "torch.save(cnn_model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8def52a",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788edc7",
   "metadata": {},
   "source": [
    "### Load the saved model paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc675e9",
   "metadata": {},
   "source": [
    "\n",
    "Loads the saved model weights back into a new CNN instance and sets it to **evaluation mode** (`eval()`), disabling features like dropout or batch norm updates to ensure consistent inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "680a3efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_layer2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_layer3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_cnn = CNN()\n",
    "loaded_cnn.load_state_dict(torch.load(path))\n",
    "loaded_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d376a",
   "metadata": {},
   "source": [
    "### Calculate accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb43391",
   "metadata": {},
   "source": [
    "Performs **evaluation** on the test dataset:\n",
    "\n",
    "- `torch.no_grad()` disables gradient computation for faster inference.\n",
    "- Loops over the test batches:\n",
    "  - Get model outputs.\n",
    "  - `torch.max(outputs, 1)` finds the predicted class index.\n",
    "  - Count correct predictions.\n",
    "- Compute and print the **accuracy** percentage over the entire test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6400d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 72.16 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = len(test_loader.dataset)\n",
    "\n",
    "    for imgs, labels in test_loader:\n",
    "\n",
    "        outputs = loaded_cnn(imgs)\n",
    "\n",
    "        _, preds = torch.max(outputs,1)\n",
    "        \n",
    "        n_correct += (preds == labels).sum().item()\n",
    "\n",
    "    \n",
    "    acc =(n_correct / n_samples) * 100.0\n",
    "    print(f\"Model Accuracy = {acc} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55479c",
   "metadata": {},
   "source": [
    "# Test on examples out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb975020",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cdb6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = new_transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: plane\n",
      "Prediction: dog\n",
      "Prediction: frog\n"
     ]
    }
   ],
   "source": [
    "images_paths  = [\"online_test_imgs/airplane_ex1.jpeg\",\"online_test_imgs/dog_ex2.jpeg\",\"online_test_imgs/frog_ex3.jpeg\"]\n",
    "images = [load_img(img) for img in images_paths]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img in images:\n",
    "        outputs = loaded_cnn(img)\n",
    "        _, pred = torch.max(outputs,1)\n",
    "        print(f\"Prediction: {class_names[pred.item()]}\")        \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
